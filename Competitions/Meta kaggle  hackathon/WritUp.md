
# I. Introduction: The Golden Age of Theory 
---
## Introduction
---
Imagine leaving a person asleep in the year 1500 and waking them up in 1700. They would notice some changes—wars, geographical shifts, and a few revolutions—but they could likely adapt. Now imagine someone falling asleep in the year 2000 and waking up in 2025. They would feel as if they had awakened in paradise—overwhelmed by the sheer speed and magnitude of technological advancement.

This writeup explores the "Golden Age of the Theory"—a period that marked the true golden era of Artificial Intelligence. At the heart of this transformation lies a groundbreaking theory: **Transformers**, introduced through the seminal paper *“Attention Is All You Need”*, submitted on **June 12, 2017**.

Based on a clear analysis of publication trends, tool releases, and model adoption across the AI community, I identified the years **2019 to 2021** as the true golden age of the Transformer theory—when it transitioned from a promising idea into a world-changing paradigm.

Why was this period so pivotal?

1. **BERT’s release in late 2018**: The first major large-scale implementation of Transformers in NLP, changing the landscape of language understanding.
2. **Hugging Face releases the Transformers library**: This open-source toolkit made advanced models widely accessible, accelerating research and deployment.
3. **The rise of pre-trained models**: Researchers began leveraging pre-trained models instead of training from scratch, drastically improving efficiency and performance.
4. **GPT-3 launches in June 2020** : With 175 billion parameters, it demonstrated that sheer model size could solve many tasks in few-shot and zero-shot settings.
5. **Transformers become a core part of NLP education**: Universities around the world started teaching Transformers as the foundation of modern NLP.
6. **Vision Transformer (ViT) is published**: Transformers crossed into Computer Vision, challenging the dominance of CNNs.
7. **OpenAI releases DALL·E and CLIP**: Stunning integrations of NLP and vision, opening the door to multi-modal generative AI.


This was the moment when abstract theory became real-world impact—when mathematical ideas like Transformers stopped being just research topics and started changing the world around us. The world didn’t just use Transformers; it was fundamentally transformed by them.

---
>**If the theory was truly transformative, shouldn’t we be able to see its fingerprints across search patterns and research activity over the years?** 
---

# II. The Flourishing of Public Attention During the The Golden Age of Theory— via Google Trends & ArXiv
---
## Google Trends: NLP vs Transformer
---
#### The Reveal: What the Clues Say

> Since its emergence, this theory has undeniably contributed to a profound transformation in the world. Its influence has extended beyond academic discourse, shaping industries, technologies, and even the way we interact with information. In order to substantiate this claim, we turned to the world's most widely used search engine—Google. By utilizing the `pytrends` library, we retrieved longitudinal search interest data related to key concepts associated with the theory. This approach allowed us to trace how public attention evolved **before the theory’s introduction**, **during its golden age**, and **in the years that followed**, providing an indirect yet compelling signal of its societal and intellectual impact.
> 
***Figure 1***
 <a href="https://ibb.co/sJRpw1hB"><img src="https://i.ibb.co/BK3HZ6m1/Figure-1-Google-Trends-NLP-vs-Transformer.png" alt="Figure-1-Google-Trends-NLP-vs-Transformer" border="0" /></a>


**Let us now take a closer look at the visualization (Figure 1)**

Focusing on the period between **2014 and early 2017**, we observe that interest in **Transformers** was virtually **non-existent**—simply because the concept had not yet **gained traction** within the **research community**. As for **Natural Language Processing (NLP)**, the field showed a **slow** and **modest growth** during this time—arguably **underwhelming** for such a **critical domain**. This **gradual rise** can largely be attributed to the increased popularity of **LSTM models** post-2014, which served as a **foundational improvement** in **language understanding**.

However, beginning in **2017**, we see a **notable shift** in the trend. Interest in both **NLP** and **Transformers** begins to **accelerate significantly**, coinciding precisely with the **introduction** of the **Transformer architecture**. Upon further inspection, we notice a **temporary decline** around **2020**, which is most likely explained by the **global outbreak of COVID-19**, an event that redirected **public and academic attention** toward understanding the **virus**, its **symptoms**, and its **risks**.

Following this **period of disruption**, the world gradually returned to **stability**, and we witnessed what can only be described as an **explosion in interest** in  **2022**—marking the **Twilight of the Golden Age of Theory**. During this phase, both **NLP** and **Transformer-related searches** **surged dramatically**, revealing a strong **positive correlation** between the two and reinforcing the idea that **Transformers** serve as the very **cornerstone of modern NLP**.

Importantly, this **explosive growth** was not **spontaneous**; rather, it was the **logical consequence** of the factors we outlined earlier in the **introduction**.


**I am aware that** I did not fully *explain* the **2021 dip**.
It may seem like a ***year of silence***—
But that silence was **deceptive**.
In fact, **2021** may have been one of the most pivotal years for the ***applied community.***
And we are about to see why.

---

>**Did Transformers alone spark the NLP revolution, or was there another, quieter force working alongside them—one that helped change everything as we know it?**

---

## Google Trends: NLP vs Transfer Learning
----
#### The Reveal: What the Clues Say
> **Transfer Learning** was a breakthrough in its own right — it opened new doors for **researchers, students, and academics alike**. At its core, the method relies on a *simple yet powerful idea*: instead of training models from scratch, one can **fine-tune a pre-trained model** — typically trained on similar data — by adjusting only the **final layers**. This **paradigm shift** was a revolution in itself, especially for **NLP**, as it combined *ease of use*, *time efficiency*, and *enhanced accuracy*. More importantly, it played a **crucial role** in the rise of the **Transformer architecture**. Given the **intensive training demands** of Transformer-based models, **Transfer Learning emerged as their primary enabler**, allowing the theory to **flourish** and **deliver on its full potential**.

***Figure 2***
<a href="https://ibb.co/SwV8bk0B"><img src="https://i.ibb.co/F48M2QJz/Figure-2-Google-Trends-NLP-vs-Transfer-Learning.png" alt="Figure-2-Google-Trends-NLP-vs-Transfer-Learning" border="0" /></a>

**Let us now take a closer look at the visualization (Figure 2)**

Upon closer inspection, we observe that the curve representing **Transfer Learning** shows a **steady** and **rapid ascent** from **2016 to 2019** — a clear indication of **growing interest** among **researchers** and **academics** in this **unique approach**. Its **consistent upward trajectory** also reflects its **expanding application** beyond **NLP**, particularly into **computer vision**, where it quickly gained ground.

In parallel, the curve for **Natural Language Processing** displays a **continuous rise**, much of which can be attributed to the emergence of **Transformers**. However, it is important to acknowledge that **Transfer Learning** acted as the **foundational enabler** of this success, making the **practical deployment** of such models **feasible**.

Interestingly, during the **COVID-19 pandemic**—when we saw a **temporary dip** in public interest in **NLP**—the curve for **Transfer Learning** remained **relatively stable**. This **resilience** suggests the method's **deeply rooted importance** in the **research community**. The real **explosion**, however, came in **2022**. As shown in *Figure 1*, both **NLP** and **Transformers** experienced a **dramatic surge**, highlighting that their success was in fact **shared**.

It a **symbiotic relationship**—much like that between a **king** and a **queen** in **chess**: the **king’s fall ends the game**, yet **without the queen, victory becomes a painful struggle**. In this analogy, the **Transformer** is the **king**—**central** and **irreplaceable**—while **Transfer Learning** is the **queen**, the **first** and **most powerful supporter**. Together, they form not just a **strategy**, but a **harmony**—as if playing in tune on the strings of a **single guitar**.

---
> **We witnessed a sudden explosion following 2021—but was this an arbitrary spike, or were there deeper forces at play?**
---

## ArXiv: The Flourishing of NLP Research
---
#### The Reveal: What the Clues Say
> **The rapid surge after 2021 was no coincidence** — it was a logical outcome, largely driven by the **proliferation** and **maturity of *real-world applications*** built upon the theory, leading to the emergence of numerous models ***beyond BERT***.
Here, we focus on one of the most **pivotal forces** behind this momentum: the ***rise in scientific publications*** during that year.

***Figure 3***
<a href="https://ibb.co/8g8J88DP"><img src="https://i.ibb.co/zVf1ffhP/Figure-3-Ar-Xiv-The-Flourishing-of-NLP-Research.png" alt="Figure-3-Ar-Xiv-The-Flourishing-of-NLP-Research" border="0" /></a>


**Let us now take a closer look at the visualization (Figure 3)**

**By closely examining this seemingly calm curve, a number of clear patterns begin to emerge.** Between *2014 and 2016*, activity remained modest—quiet ripples before the tides shifted. But a closer look beyond **2016** reveals a steady and noticeable rise in the number of NLP publications, peaking sharply around *2019*—marking the dawn of **The Golden Age of Theory**. That spike coincides with the release of **BERT**, the most powerful and influential Transformer-based model at the time, which naturally triggered a surge in academic interest and output.

Surprisingly, the upward trend continued even in ***2020***, defying expectations amidst the global disruption of the ***COVID-19*** pandemic—highlighting the resilience and commitment of the research community. Then came ***2022***, with a staggering **8,971 papers** published—an overwhelming testament to the field’s momentum and the researchers’ growing engagement.

Notably, ***2021*** also witnessed a remarkable output of **8,083 papers**, bringing the **combined total for 2021 and 2022 to 17,054**. This volume of research isn’t just impressive in scale—it represents a critical mass, a sufficient and powerful foundation upon which real-world implementation of these ideas could begin in earnest.

From that point onward, the explosion only intensified, culminating in *2024*, which stands as the highest peak yet—an apex driven by the deep-rooted integration of **Transformer theory**, which by then had become the **foundational engine** of effective language understanding.

---
## Framing the Picture: Insights We’ve Reached So Far:
---
Based on the analysis, several clear patterns and trends have emerged:

1. **Transformers have demonstrated a greater and more profound impact** on language processing than any previous neural architecture. Their influence on the field was evident through Google Trends and other public attention signals.

2. **Transfer Learning played a critical supporting role**, working in tandem with Transformers. Together, they accelerated the adoption and popularity of NLP applications, with Transfer Learning acting as a key enabler for Transformer-based models.

3. **A consistent pattern emerged**: a steady rise in interest from the introduction of the theory until **2019**, followed by a dip in **2020**, and then an explosive resurgence in **2022**.

4. **Analyzing the research community**—***the community that teaches how to fish rather than simply giving the fish***—reveals a pattern that closely mirrors the trajectory of progress.
With each successful paper comes a ripple effect: successful applications follow, fueling even more research.
This **cyclical dynamic** is clearly reflected in both **arXiv** and the ***ACL conference***, where academic momentum and practical impact move hand in hand.


5. The late-2021 surge can be attributed to multiple factors, including:

   * A renewed wave of research and a return to publishing on platforms like **[ACL](https://aclanthology.org/events/acl-2021/)** *and* **arXiv**
   * The widespread use of **GPT-3**, which represented a leap not just for NLP but for machine learning as a whole.
   * The rapid rise and popularity of **[Hugging Face](https://huggingface.co/docs/transformers/en/model_doc/auto)**, which significantly lowered the barrier to accessing and using pre-trained models.
   * The emergence of the concept of **[Prompt Engineering](https://arxiv.org/abs/2104.08691)**, which redefined how models interact with users. 
   * The launch of **[GitHub Copilot](https://github.blog/news-insights/product-news/introducing-github-copilot-ai-pair-programmer)**, powered by GPT-3, marking the first large-scale assistant built on **LLMs**.

---
>***But did this theory truly reshape the applied community—Kaggle? And if so, how deeply did its impact run through the kernels, frameworks, datasets, and even the social fabric of the Kaggle community itself?*** 
**And will we realize that 2021 wasn’t actually a downturn, as it seemed?**
---

# III. The Flourishing of Kernels
---
## The Flourishing of Applied NLP Practices
---
#### The Reveal: What the Clues Say
> After such a surge in publications and the world’s deep immersion in a ***new analytical culture***, it was only natural that humans would not keep inventing without applying.
**After all, can a drug be called a cure without being tested and practiced?**
And so, we now dive into the evolution of **Kaggle notebooks**—a space where ***theory meets practice***, and ideas are turned into ***real-world workflows***.

***Figure 4*** 
<a href="https://ibb.co/cS0bS4H8"><img src="https://i.ibb.co/mVkvVfwy/Figure-4-The-Flourishing-of-notebooks.png" alt="Figure-4-The-Flourishing-of-notebooks" border="0" /></a>

**Let us now take a closer look at the visualization (Figure 4)**

***Whispers Before the Storm — When the Field Was Just Warming Up***

After mapping each notebook to its relevant tags and clustering them into four core domains—**Natural Language Processing**, **Computer Vision**, **LSTM architectures**, and **Transformers**—clear and emotionally charged patterns began to emerge.
If we start with **2017**, the landscape appears remarkably sparse: **fewer than 250 notebooks** were shared that year, a number so low that their impact was nearly invisible. NLP led the scene modestly, with a handful of LSTM-based models. **Transformers were completely absent**, and Computer Vision activity was almost negligible.

As we move into **2018–2019**, we observe a gentle shift. Notebooks doubled in quantity, and interest in **LSTM architectures** began to rise slightly. This corresponded with a **moderate growth in NLP**, but not yet a disruptive one—more of a quiet buildup, like pressure before the storm.


***The Catalyst Years — When Transformers Lit the Fuse***

By **2020**, momentum was building. A subtle increase in Transformer-tagged notebooks suddenly translated into a ***massive spike in NLP adoption***. This wasn’t just a linear rise—it was an explosion.
Even **LSTM usage rose sharply**, likely due to its frequent hybridization with Transformers in applied workflows.

Then came **2021**—the unmistakable **zenith**. Transformer models soared in practical adoption, which **directly fueled an NLP boom** like never before.
The relationship was clear: ***Transformers didn’t just enhance NLP—they ignited it.***
At the same time, even if Transformers were not used directly in all notebooks, their **ripple effect** elevated the entire ecosystem—including LSTM awareness and language modeling as a whole.


***Vision Finds Its Voice — The Ripple Effect Reaches Images***

A parallel but no less impressive rise occurred in **Computer Vision notebooks**.
This, too, was influenced by NLP’s booming popularity—especially as tasks like **image captioning** and **text extraction from images** became common. But another factor played a pivotal role: the **emergence of Vision Transformers (ViT)**.
This architecture caught the attention of the applied ML community, seamlessly transferring Transformer principles to the visual domain, and giving practitioners new ground to explore.

***Normalization or Mastery? — The Calm That Follows Revolution***

Ultimately, **Transformers replaced LSTM** as the dominant paradigm—not by decree, but by merit. Their flexibility, scalability, and results proved **irresistible**.
By 2022 onward, the declining novelty in the notebook charts doesn’t imply stagnation, but rather ***normalization***: practitioners had **fully internalized NLP and CV practices**, thanks to the **maturity of Transformers**.

And even beyond technical notebooks, the ripple effect of Transformers was felt **worldwide**.
The surge in public-facing **chatbots**, especially the rise of ***ChatGPT***, was the ultimate testimony that this theory didn’t just transform academic models—it reshaped global interaction.
In retrospect, **Transformers were not merely an improvement—they were the *turning point*.**

---
>**But were these notebooks truly valuable and well-received, or were they just a flood of content that went unnoticed?**
*Did they truly capture interest, or were they merely published in vain?*
---

## Notebook Viewership Flourishing in the Theory’s Golden Age
---
#### The Reveal: What the Clues Say
>**Most certainly, these notebooks were far from meaningless — they did capture real attention.**
***And now, we’re about to see exactly how.***

***Figure 5***
<a href="https://ibb.co/MxbDqGhs"><img src="https://i.ibb.co/B5mV8rPt/Figure-5-Notebook-Viewership-Flourishing-in-The-Golden-Age-of-Theory.png" alt="Figure-5-Notebook-Viewership-Flourishing-in-The-Golden-Age-of-Theory" border="0" /></a>

**Let us now take a closer look at the visualization (Figure 5)**

 If we **dive into the plot** more carefully, we begin to notice ***clear and consistent patterns***. But **before exploring the deeper trends**, it's essential to note that the **values on the Y-axis are scaled in millions**—so a value like **7 actually means 7 million visits**. This is crucial for accurate interpretation.

Upon closer inspection, we observe that the **Computer Vision curve slightly precedes** that of **Natural Language Processing (NLP)**. However, ***this precedence holds little analytical value***, as the real story unfolds in **2017**, where ***both fields experienced a sharp rise***. By **2018**, we witness the ***first major peak***—with **NLP clearly surpassing Computer Vision**. This aligns with earlier discussions highlighting that, **despite its earlier emergence, Computer Vision was overshadowed by the explosive growth in NLP interest.**

In **2019**, we see an ***unexpected drop*** in traffic. But by referencing **Figure 4**, it's clear that **2018 and 2019 had very similar activity levels**, indicating a period of **stagnation in downloads**, which naturally resulted in **fewer visits and reduced attention**.

Moving into the **midst of The Golden Age of Theory**, the plot reveals a ***staggering surge—from approximately 2 million to over 7 million visits!*** This is a compelling indication that ***language analysis was reshaping the world at that time***, thanks to the rise of **Transformers**. Moreover, **Computer Vision also experienced a resurgence**, likely tied to the growing interest in ***text extraction from images and related tasks***.

Now, when we place this *chart* side by side with **Figure 4**, we clearly observe a drop in public interest during **2021.**
**However**, what stands out is the **significant rise in the number of shared Notebooks on Kaggle during the same period**.
This indicates a logical shift: **after the unprecedented spike in visits in 2020**, many of the **observers** who were merely **browsing or exploring the field** turned into actual ***contributors and practitioners in 2021.***
**In other words, the drop in visits doesn't signal a decline in engagement—but rather a transformation in the type of engagement.**
***This same pattern continued in the years that followed.***


Ultimately, this plot serves as ***strong evidence that the boom in Notebooks during The Golden Age of Theory was not arbitrary or without purpose.*** On the contrary, these notebooks ***garnered significant attention and represented a true shift in the community’s engagement and innovation.***

---
>**But what frameworks were powering all this momentum?** **And** did one ***framework*** begin to attract more attention than the ***others***?
---

## TensorFlow vs. PyTorch Popularity During the Theory’s Golden Age
---
#### The Reveal: What the Clues Say
> **There must be supporting and driving frameworks for deep learning in general—and more specifically, in language and vision analysis.**
These frameworks have been centered around two: the **first and older**, ***TensorFlow***, and the **second and newer,** ***PyTorch***.
They are the two poles of deep learning in the world, and now we’ll dive into analyzing them.


***Figure 6***
<a href="https://ibb.co/vCmghqmQ"><img src="https://i.ibb.co/qL9XCj9W/Figure-6-Tensor-Flow-vs-Py-Torch-Popularity-During-The-Golden-Age-of-Theory.png" alt="Figure-6-Tensor-Flow-vs-Py-Torch-Popularity-During-The-Golden-Age-of-Theory" border="0" /></a>

**Let us now take a closer look at the visualization (Figure 6)**

**Upon closer inspection of the chart, we observe that TensorFlow indeed predates PyTorch—just as we highlighted earlier.**

Looking at the general trend, we can see that **TensorFlow's curve experienced a slight rise after 2018**—not a dramatic one, but a rise nonetheless. At that point, **PyTorch had just entered the scene**, which was still largely dominated by TensorFlow.

As we zoom in further, we find that **after 2019, not much changed in TensorFlow’s activity**, while **PyTorch showed a modest increase**. But at the critical moment, we notice something striking:

***A pyramid was built in the chart—and its builder was TensorFlow.***
During that same period, PyTorch also rose slightly, but nothing compared to the surge seen in TensorFlow.

Now, a deeper analytical look reveals **why this explosive growth occurred in TensorFlow rather than in PyTorch**, and this can be traced back to several key factors.

Most notably, it’s fascinating to realize that the legendary paper *“Attention Is All You Need”*—which laid the foundation for the theory itself—was **originally implemented using TensorFlow**.
In fact, one of the primary reasons for **TensorFlow’s fame during that age** is that the **central model BERT was built using it**. BERT had a **tremendous impact on the rise of the theory**, as it represented its most powerful practical application at the time.

Another contributing factor was the **abundance of tutorials**, both on BERT and deep learning in general, which often **relied on TensorFlow as the teaching framework**. On top of all this, TensorFlow was simply **older and more established than PyTorch**.

Moving slightly into the future, we see a **sharp decline in TensorFlow’s curve after 2021**, and while PyTorch also declined, the drop was **less severe**.
This leads us to a crucial question:

***Was TensorFlow dominant for a time? Or truly dominant in an absolute sense?***

The truth is, **no community fully agrees on a single answer**, but what’s clear is that **TensorFlow was indeed dominant—until around 2021**, for all the reasons we just outlined.
So, why did it lose momentum? And why did **PyTorch begin to surpass it**?

The answer lies in what happened in 2021:
That year marked the **global rise of GPT**, and **all OpenAI research was built using PyTorch**. GPT was a **massive leap forward—even more influential than BERT**.
Interestingly, even **BERT itself was later reimplemented in PyTorch**.

Another **turning point** was the release of the **Transformers library by Hugging Face**, which **revolutionized access to pre-trained models** and became a central force in the adoption of PyTorch.

In the end, **TensorFlow was like the wise old teacher**, who led the way in the beginning—only to be succeeded by **the diligent student, PyTorch**, who took things even further.

---
> But after the **explosion of Transformers** and the rise of the **theory—**did this seismic shift impact the ***NLP subfields themselves?*** Did it **open doors** *to new*, **unconventional directions**
---

## The Flourishing of NLP Subfields During the Theory’s Golden Age — An Analysis of Research Publication Trends on arXiv
---
#### The Reveal: What the Clues Say
> There’s no doubt that the theory transformed the way we **approach language analysis**—and *soon after*, it began to **reshape** the world itself through models like ***ChatGPT***, which are entirely built upon it.
*However*, all our analysis so far has **focused on Natural Language Processing as a whole**.
**But what about the subfields within NLP?**
That’s the **question** we’ll now begin to **explore**.

***Figure 7***
<a href="https://ibb.co/355W3Txf"><img src="https://i.ibb.co/Pss4Lr2T/Figure-7-NLP-Subfields-Growth-During-The-Golden-Age-of-Theory-Ar-Xiv.png" alt="Figure-7-NLP-Subfields-Growth-During-The-Golden-Age-of-Theory-Ar-Xiv" border="0" /></a>

**Let us now take a closer look at the visualization (Figure 7)**

What we observe in the figure is a trend that grows in a **smooth and elegant sequence**. Upon calling on a keen eye, **fascinating patterns** begin to emerge. From **2014 to 2017**, the number of research papers was relatively modest, showing a **gradual upward trajectory**. Yet, more significant than this overall rise is the **clear dominance of the field of *Text Classification*** during this phase. Another **noteworthy observation** is the **low presence of *Text Generation***—a subfield that would later become **world-changing**. Meanwhile, we also see a **silent soldier rising steadily in the background**: ***Question Answering***.

It is **not surprising** that *Text Classification* held an early lead, as it is a **relatively simpler task**, while the other subfields represent **greater complexity**. These complex tasks were waiting for a driving force—and indeed, it was **THEORETICAL ADVANCEMENTS**, particularly **transformer architectures**, that would unlock their full potential.

From **2018 to the end of The Golden Age of Theory**, we observe a **clear surge in *Question Answering***. This is understandable, as it is a task that serves a **wide range of applications**: from **search engines** to **chatbots**, **FAQ systems**, and **virtual assistants**. At the same time, *Text Classification* continued to rise, highlighting the **broad impact of transformers** across both simple and complex tasks. ***Text Generation***, in particular, witnessed **remarkable growth**, a trend that will become even more apparent.

After the **The Golden Age of Theory**, in the following years, *Text Classification* begins to **decline relative to other tasks**—a **natural progression**. As transformers **simplified** this task considerably, attention began shifting towards **more complex tasks**. The **transformer architecture’s ability to assign attention weights** enabled it to **meet the intricate demands** of these advanced tasks, propelling their growth.

---
> Well, **this is within the *research community*** —
but what about the ***applied world*?**
---

## The Flourishing of NLP Subfields During the Golden Age of Theory - Practical Applications at Kaggle
---
#### The Reveal: What the Clues Say
> After the **explosive growth** and **peak in the number of research papers** across the **subfields of Natural Language Processing**,
it is only natural to witness these **research interests seamlessly spilling over** into the **applied world**.

***Figure 8***
<a href="https://ibb.co/qYC7R2dv"><img src="https://i.ibb.co/wh7BK2Rf/Figure-8-Flourishing-of-NLP-Subfields-in-The-Golden-Age-of-Theory-on-kaggle.png" alt="Figure-8-Flourishing-of-NLP-Subfields-in-The-Golden-Age-of-Theory-on-kaggle" border="0" /></a>

**Let us now take a closer look at the visualization (Figure 8)**

When I first looked at this figure, I couldn’t help but wonder: *Why does the shape look like this?* One would expect to see **curves, spikes, and dips**—a dynamic landscape. Yet, what we observe instead is **a long, unsettling silence** stretching from **2018 to around 2021**. A collective stillness.

As I dug deeper, I found a **justifiable reason** for this unusual quiet.

Let’s step back for a moment and revisit **Figure 4**. We notice a clear **explosion in NLP-related notebooks** during that same period. This highlights something crucial:
the primary focus at the time wasn't on **developing specific subfields** within NLP, but rather on **exploring NLP itself as a technology**.

Back then, **labeling each notebook with its exact NLP subfield** was not a priority. Simply tagging it as “*nlp*” was deemed sufficient—even if the notebook was, in practice, dedicated to a particular task. The **classification and formal subfield labeling** were **overlooked**, as the emphasis was on **learning and experimentation**, not taxonomy.

Once subfield labeling began in earnest, we see **Text Classification** emerging with a noticeable spike—a logical outcome, given that it is a **relatively simpler task**.
However, as we move forward, two **warriors** rise fast and powerfully.

Upon closer inspection, the **red warrior** (*Text Generation*) proves to be a **rapid force**. Within just two years, it **surpassed Text Classification**, which is a clear indicator of its **rising importance**—especially following the rise of **GPT**, a paradigm-shifting innovation that redefined the entire field of AI.

Then there's the **purple warrior** (*Question Answering*), which appears quieter—**less explosive** than one might expect based on its research trajectory. But this too can be attributed to **mislabeling** and **lack of consistent categorization**. Still, despite the subdued appearance, it maintained a **steady and accelerating presence**.

In the end, this figure—though seemingly flat and unusual at first glance—**reveals a familiar pattern**:
a period of **intensive applied interest**, but **poor subfield annotation**, leading to an underrepresentation of the true activity within each task.
It serves as a **mirror** to a common phenomenon in practical machine learning: **the excitement of application often precedes the precision of categorization**.

---
## Framing the Picture: Insights We’ve Reached So Far:

**Based on the analysis, several clear patterns and trends have emerged:**

1. ***The Applied Community: A Parallel Force in The Golden Age of Theory***

The applied NLP community evolved **in parallel and in strength** alongside the research community—
a dynamic we observed clearly in earlier analyses.
Theoretical advancements were not confined to academia; their influence extended deeply into real-world systems and applications.



2. ***Beyond Language: Theoretical Breakthroughs in Computer Vision***

The rise of theoretical models—particularly transformer architectures—was not limited to Natural Language Processing.
Their **impact reached into Computer Vision as well**, as illustrated in **Figures 4 and 5**.
Key contributors to this cross-domain influence include the emergence of **Vision Transformers (ViT)** and the growing interest in tasks such as **text extraction from images**, among others.


3. ***The Framework Wars: From TensorFlow’s Rise to PyTorch’s Resurgence***

We also witnessed a **notable evolution in deep learning frameworks**.
Initially, **TensorFlow experienced a surge**, outpacing PyTorch.
However, PyTorch would soon regain momentum—a shift we have since analyzed in depth.



4. ***Subfield Dynamics in Research: The Rise of Generation and QA***

Within the research community, we saw a **significant rise in interest in *Question Answering***, for reasons previously discussed.
Likewise, ***Text Generation*** garnered growing attention, while ***Text Classification*** began to lose its dominance.
These trends were clearly reflected in both publication volume and methodological focus.



5. ***A Practical Dilemma: Mislabeling and the Focus on Application***

Upon shifting to the applied domain, a key issue came to light:
**the mislabeling and inconsistent classification of NLP subfields**, with the primary focus directed toward **practical implementation** rather than methodological clarity.

6. ***2021: The Silent Engine Behind the Surge***
   
After analyzing all of this—and what is yet to come—it becomes clear that **2021 was an underrated year on Google Search**, yet **an extremely significant one within both the applied and research communities**, as we clearly observed in **Figure 3**.                           
And Most of the breakthroughs **happened during it**, and it acted as **the spark and catalytic driver** behind **the explosive developments of 2022, which later became highly visible on Google Trends**.



Nevertheless, clear patterns emerged—*Text Classification* initially received considerable attention, but gradually gave way to the **red warrior** (*Text Generation*), while the **purple warrior** (*Question Answering*) remained underrepresented, **contrary to expectations**.
This discrepancy, once again, circles back to the core problem we identified: **inaccurate tagging and lack of proper subfield attribution** within applied work.


---

>***After all this progress, one may wonder:
How is it decided that one model excels while another falls short?
It is the presence of a shared ground—a silent standard—
the very stone upon which success is measured.
But during The Golden Age of Theory… did that foundation itself evolve?***

---

# IV. The Flourishing of Datasets During The Golden Age of Theory
 
 ---
#### The Reveal: What the Clues Say
>Naturally, advancements in Natural Language Processing could not have unfolded without a parallel rise in the datasets that powered this growth.
Without the **widespread availability and diversity** of supporting datasets, progress would have stalled—
for nothing meaningful can emerge in a vacuum.
>
>Datasets offer us the **freedom to explore**, the **space to compare**, and most importantly, the ability to **experiment**.
If one is to claim that **transformers outperform LSTMs**, such a claim must be grounded in **experimentation on the same datasets**, under the same conditions.
>
>In this way, datasets serve as a **silent calibrator**—an indirect yet essential standard that underpins fair comparison.
That is precisely why we witnessed **their flourishing during The Golden Age of Theory**.

***Figure 9*** 
<a href="https://ibb.co/5hK9tXDt"><img src="https://i.ibb.co/d4mc1s91/Figure-9-Dataset-Flourishing-During-the-The-Golden-Age-of-Theory.png" alt="Figure-9-Dataset-Flourishing-During-the-The-Golden-Age-of-Theory" border="0" /></a>

**Let us now take a closer look at the visualization (Figure 9)**



If we pause for a moment and look closely, the curve speaks for itself — there's hardly a need for intense analysis. The patterns are *loud and clear*.

From **2017 to 2019**, the scene was calm. Understandably so — the available datasets hovered around the **20**, which is more than enough for tackling basic *NLP tasks*. *No chaos*, no *explosion*. Just stability.

But then comes the *turning point* — **2019 to 2021**.
An *explosion* unfolds before *our eyes*, with a staggering **1644% growth**. That’s not just growth — that’s a *detonation*. The chart itself *draws a sharp*, *almost vertical spike*, and it’s not a *coincidence*.

We can flip the logic here: it’s not that **notebooks or subfields** caused this boom — it’s the other way around. This dataset boom *unlocked* the rest.

Think of it this way:

**The more fertile the land, the richer the harvest.**
In this metaphor, **the datasets are the soil**. The output — **notebooks,innovations** — are the fruit. The richer the soil, the more apples we get. And clearly, this was a harvest season.

**This explosion** is also tightly interwoven with the **rise of transformers** — their spread and entrenchment in the community. As we said before, *many threads lead here*, but one thing’s for sure: ***without data, nothing grows.***

---
>Were these **datasets** *well-known* and *well-received?*,
And what about **computer vision** *datasets?*
---

## Dataset Download Flourishing in the Theory’s Golden Age
---
#### The Reveal: What the Clues Say
> Certainly, these datasets have indeed garnered considerable attention; however, I have opted to rely on a more robust indicator—**TotalDownloads**—as concrete evidence of their impact. This metric offers a quantifiable measure of interest and utilization. Furthermore, we shall compare this with the level of attention received by datasets in the field of **computer vision**, providing a broader context for evaluation.

***Figure 10***
<a href="https://ibb.co/vxcjGC3X"><img src="https://i.ibb.co/pv1L86fh/Figure-10-NLP-vs-CV-Dataset-Downloads-During-The-Golden-Age-of-Theory.png" alt="Figure-10-NLP-vs-CV-Dataset-Downloads-During-The-Golden-Age-of-Theory" border="0" /></a>

**Let us now take a closer look at the visualization (Figure 10)**

As we **delve into the figure**, we begin to observe **clear and compelling patterns**. The first of these becomes evident when we **return to Figure 9**, focusing on the period **before The Golden Age of Theory**. In **2017**, there were **no clear patterns** in either of the plots. However, by **2018**, although Figure 9 shows only a **very slight increase**, the **download count reveals a noticeable rise in interest**.

**This alone serves as strong and sufficient evidence that the world was in need of an explosion in the availability and diversity of datasets.**

With the **advent of The Golden Age of Theory**, we witness exactly what was anticipated: **a massive surge in dataset downloads**. This sharp rise further **reinforces our claim** that the global research community was **eagerly awaiting a broader pool of text data**.
Indeed, **the number of downloads exceeded 3.5 million**, a **remarkably high figure**, underscoring the **intense demand** at that time.

The **decline after 2020** is, in fact, **expected**, since those who downloaded datasets in 2020 **had little need to re-download in 2021**, especially considering that **textual data diversity was still limited**. This conclusion is supported by our **analysis of NLP subfields** **(Figure 8)** , which confirmed that **awareness of these subdomains remained relatively limited**.

Moreover, revisiting **Figure 9**, it becomes evident that **the true boom in dataset creation occurred in 2020—not 2021**—further validating our interpretation.

Turning to the **faint red curve**, which represents **computer vision datasets**, we notice that **prior to the theoretical boom**, it was **steadily and smoothly growing** with no significant drops. However, as we enter the **The Golden Age of Theory**, we observe a **clear uptick in visits to computer vision datasets**.

This presents **strong evidence** of a **growing interest** in **extracting textual information from visual data**, signaling a **convergence between computer vision and natural language processing**.

---

## Framing the Picture: Insights We’ve Reached So Far
**Based on the analysis, several clear patterns and trends have emerged:**

1. ***When Data Isn’t Enough: The Community’s Thirst for Richer Datasets***

As shown in **Figure 10**, there was a notable enthusiasm for the available datasets **despite their limited number**. This supports our earlier observation that the community, at the time, was ***evidently thirsty for datasets*** — seeking the ***freedom to experiment*** and **choose according to its needs and preferences**.

3. ***All Eyes on the Data: How the NLP Community Reacted to the Newcomers***

 As shown in **Figure 10**, we observed that the community showed ***intense interest in these datasets***. Once a large number of datasets were released — as illustrated in **Figure 9** — **they received remarkable attention**, **especially when compared to computer vision and other fields at the time.**


4. ***More Than Just Text: The Strategic Power of NLP Datasets***

The **role of datasets** was far from *ordinary* — they served as ***the foundation and the very ground for experimentation***. After all, a farmer cannot cultivate the land, even if they possess all the seeds in the world, without a piece of soil. In this analogy, **datasets are the fertile ground**, while ***theories and research are the seeds***.

---
>But what about the **reaction of the Kaggle community itself**—how did it respond to all of this?
---

# V. The Flourishing of the Kaggle Community
---
## The Flourishing of Forum Topics During The Golden Age of Theory
---
#### The Reveal: What the Clues Say
> Certainly, it is only natural to expect a reaction from the Kaggle community—**and indeed, we witnessed it**: the **sharing of notebooks**, **the growing attention to them**, and similarly, **the interest in datasets**.
**But what about Topics?**

***Figure 11***
<a href="https://ibb.co/bRgH1hRr"><img src="https://i.ibb.co/R4ph2n4C/Figure-11-NLP-vs-CV-Formula-Topics-During-The-Golden-Age-of-Theory.png" alt="Figure-11-NLP-vs-CV-Formula-Topics-During-The-Golden-Age-of-Theory" border="0" /></a>

**Let us now take a closer look at the visualization (Figure 11)**

As we **dive into the figure**, we begin to observe patterns that—**to the oue eyes—were expected all along**. When examining the **forumTopics** data, a clear and anticipated trend emerges: the **natural language processing (NLP) curve surpasses that of computer vision**. This comes as no surprise.

However, when tracing this trajectory from **2018 to the beginning of the golden era**, we find that **NLP discussions began to emerge and gain traction** shortly after their first appearance—even though **computer vision had been present for longer**. Yet, **as is often the case, NLP once again took the lead**.

In **2020**, we witness **an explosive rise in both curves**, which can be explained by several factors mentioned in our introduction. Among them are major breakthroughs such as the release of **T5 by Google**, which—despite being published in 2019—**gained widespread recognition in 2020**. Similarly, **Facebook's [BART](https://arxiv.org/abs/1910.13461) model**, a hybrid between **BERT and GPT**, sparked significant interest.
These **transformative models** triggered waves of **discussions, topics, and community engagement** across the platform.

Looking more closely at the **post-golden-age period**, we observe a **clear decline in NLP-related forum activity** and, interestingly, a **flourishing of computer vision discussions around 2022**. This shift can be traced back to **groundbreaking innovations** in the vision domain that captured the community's attention.

Among the most influential developments was the launch of **[DALL·E 2](https://openai.com/index/dall-e-2/) by OpenAI**, which marked a revolution in **text-to-image generation**. Other notable milestones include the release of **[PaLI](https://arxiv.org/abs/2209.06794)**, the growing popularity of **diffusion models**, and the introduction of **[Stable Diffusion](https://github.com/CompVis/stable-diffusion)** and **Imagen**.
**All of these contributed to a surge of interest in computer vision discussions during 2022.**

Following that spike, we observe the **inevitable decline**, which can be attributed to the **community gradually adapting to these groundbreaking technologies**, leading to a natural leveling-off of excitement.

---
# VI. Bringing It All Together: The Big Picture Emerges
---
In this write-up, we have  **traced and analyzed  _The Golden Age of Theory_**, examining its  **profound impact on scientific inquiry**  through platforms like  **Google Trends**  _and_  **ArXiv**. We also explored the  **conceptual foundations**  that supported this era, such as  **transfer learning**.

We then followed the  **transformations within the applied community**  —  _(Kaggle)_  — starting from the  **evolution of notebooks**, moving through the  **development of frameworks and datasets**, and finally and finally to the  **forum topics**.

From the outset,  _**my vision was clear**_: this era was  **not merely a technical phase**, but a  **pivotal moment**  in which  _**theory reached the peak of its influence and recognition**_. I supported this perspective with ***data, embedded references, and well-documented sources**  —ensuring that  **every conclusion was grounded in evidence or visible patterns.**

I have always believed that this theory,  **especially in its golden age**, didn’t just  _**reshape artificial intelligence… it began to reshape the world itself.**_

**We also came to see that  _2021 was somewhat overlooked in Google Trends_**, yet in reality—within both the  **research**  and  **applied**  communities—it was  **far from an ordinary year**. In fact,  **much of the explosive momentum**  we observed actually  **took root during that time**. This reminds us of the importance of  **looking beyond a single perspective**, and ensuring our comparisons are  **comprehensive**  and  **multi-dimensional**.

And perhaps, one day,  **we will chronicle the next revolutionary theory**  — the one that will lead us into  **AGI (Artificial General Intelligence)**.

_**Is that day near?**_









